{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from xgboost import plot_importance\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from datetime import date,datetime\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "traindata=pd.read_csv('../dataset/fifa2018/train.csv')\n",
    "testdata=pd.read_csv('../dataset/fifa2018/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 293, 25, ..., 53.0, 56.0, nan],\n       [1, 258, 24, ..., 61.0, 64.0, nan],\n       [2, 112, 3, ..., 31.0, 36.0, nan],\n       ...,\n       [10438, 626, 26, ..., 41.0, 46.0, nan],\n       [10439, 147, 9, ..., 49.0, 50.0, nan],\n       [10440, 234, 18, ..., 68.0, 63.0, nan]], dtype=object)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "traindata.values[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3a5b305c9c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraindata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rw'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'st'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'traindata' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'traindata' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "traindata[['rw','rb','st']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-436de73e6ed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtoday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtraindata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'birth_date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'birth_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'birth_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "today = datetime(2018, 4, 15)\n",
    "traindata['birth_date'] = pd.to_datetime(traindata['birth_date'])\n",
    "today-traindata['birth_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2a5895e6e847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraindata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'traindata' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'traindata' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "traindata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputData():\n",
    "    def __init__(self, train_data_path, test_data_path, split_ratio=0.2):\n",
    "        self._train_data_path = train_data_path\n",
    "        self._test_data_path = test_data_path\n",
    "        self._split_ratio = split_ratio\n",
    "        np.random.seed(42)\n",
    "        self._train_data = None\n",
    "        self._eval_data = None\n",
    "        self._test_data = None\n",
    "\n",
    "    def _add_features(self, data):\n",
    "        today = datetime(2018, 4, 15)\n",
    "\n",
    "        data['birth_date'] = pd.to_datetime(data['birth_date'])\n",
    "        data['age'] = (today - data['birth_date']).apply(lambda x: x.days) / 365.\n",
    "        data['BMI'] = 10000. * data['weight_kg'] / (data['height_cm'] ** 2)\n",
    "        data['is_gk'] = data['gk'] > 0\n",
    "\n",
    "        positions = ['rw', 'rb', 'st', 'lw', 'cf', 'cam', 'cm', 'cdm', 'cb', 'lb', 'gk']\n",
    "\n",
    "        data['best_pos'] = data[positions].max(axis=1)\n",
    "        data['best_pos'] = data[positions].max(axis=1)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _split_data(self, data):\n",
    "        if self._split_ratio <= 0:\n",
    "            return data, None\n",
    "        shuffled_indices = np.random.permutation(len(data))\n",
    "        test_set_size = int(len(data) * self._split_ratio)\n",
    "\n",
    "        test_indices = shuffled_indices[:test_set_size]\n",
    "        train_indices = shuffled_indices[test_set_size:]\n",
    "        return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "    def _load_train_data(self):\n",
    "        df = pd.read_csv(self._train_data_path)\n",
    "        data = self._add_features(df)\n",
    "        self._train_data, self._eval_data = self._split_data(data)\n",
    "\n",
    "    def _load_test_data(self):\n",
    "        df = pd.read_csv(self._test_data_path)\n",
    "        self._test_data = self._add_features(df)\n",
    "\n",
    "    def train_data(self, is_gk):\n",
    "        if self._train_data is not None:\n",
    "            return self._train_data[self._train_data['is_gk'] == is_gk]\n",
    "        self._load_train_data()\n",
    "\n",
    "        return self._train_data[self._train_data['is_gk'] == is_gk]\n",
    "\n",
    "    def eval_data(self, is_gk):\n",
    "        if self._eval_data is not None:\n",
    "            return self._eval_data[self._eval_data['is_gk'] == is_gk]\n",
    "\n",
    "        self._load_train_data()\n",
    "        return self._eval_data[self._eval_data['is_gk'] == is_gk]\n",
    "\n",
    "    def test_data(self, is_gk):\n",
    "        if self._test_data is not None:\n",
    "            return self._test_data[self._test_data['is_gk'] == is_gk]\n",
    "        self._load_test_data()\n",
    "        return self._test_data[self._test_data['is_gk'] == is_gk]\n",
    "\n",
    "    def all_test_data(self):\n",
    "        if self._test_data is not None:\n",
    "            return self._test_data\n",
    "        self._load_test_data()\n",
    "        return self._test_data\n",
    "\n",
    "    def has_eval(self):\n",
    "        return self._eval_data is not None\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, model_builder, input_data: InputData, is_gk, features=None):\n",
    "        self.model = model_builder()\n",
    "        self.features = features\n",
    "        self.input_data = input_data\n",
    "        self.is_gk = is_gk\n",
    "\n",
    "    def fit(self):\n",
    "        train_data = self.input_data.train_data(self.is_gk)\n",
    "        self.model.fit(train_data[self.features], train_data['y'])\n",
    "\n",
    "    def eval(self):\n",
    "        if self.input_data.has_eval():\n",
    "            eval_data = self.input_data.eval_data(self.is_gk)\n",
    "        else:\n",
    "            eval_data = self.input_data.eval_data(self.is_gk)\n",
    "        if eval_data is None:\n",
    "            return None, None\n",
    "        return self.model.predict(eval_data[self.features]), eval_data\n",
    "\n",
    "    def test(self):\n",
    "        test_data = self.input_data.test_data(self.is_gk)\n",
    "        if test_data is None:\n",
    "            return None, None\n",
    "        return self.model.predict(test_data[self.features]), test_data\n",
    "\n",
    "\n",
    "class SplitGKModel():\n",
    "    def __init__(self, model_builder, input_data: InputData, gk_features=None, not_gk_features=None):\n",
    "        self.gk_model = Model(model_builder, input_data, True, gk_features)\n",
    "        self.not_gk_model = Model(model_builder, input_data, False, not_gk_features)\n",
    "        self.input_data = input_data\n",
    "\n",
    "    def fit(self):\n",
    "        self.gk_model.fit()\n",
    "        self.not_gk_model.fit()\n",
    "\n",
    "    def evaluate(self):\n",
    "        prediction1, eval_data1 = self.gk_model.eval()\n",
    "        prediction2, eval_data2 = self.not_gk_model.eval()\n",
    "        sum = np.abs(eval_data1['y'] - prediction1).sum() + np.abs(eval_data2['y'] - prediction2).sum()\n",
    "        return sum / (len(eval_data1['y']) + len(eval_data2['y'])), prediction1, prediction2\n",
    "\n",
    "    def test(self):\n",
    "        prediction1, eval_data1 = self.gk_model.test()\n",
    "        prediction2, eval_data2 = self.not_gk_model.test()\n",
    "        return prediction1, prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model_builder, gk_features=None, not_gk_features=None, eval_ratio=0.2):\n",
    "        self.train_file_path = '../dataset/fifa2018/train.csv'\n",
    "        self.test_file_path = '../dataset/fifa2018/test.csv'\n",
    "        self.submit_file_path = '../dataset/fifa2018/sample_submit.csv'\n",
    "        self.prediction_file_path = '../dataset/fifa2018/prediction.csv'\n",
    "        if not_gk_features is None:\n",
    "            not_gk_features = gk_features\n",
    "\n",
    "        self.input_data = InputData(self.train_file_path, self.test_file_path, eval_ratio)\n",
    "        self.model = SplitGKModel(model_builder, self.input_data, gk_features, not_gk_features)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.fit()\n",
    "        result, gk_preds, not_gk_preds = self.model.evaluate()\n",
    "        return result, gk_preds, not_gk_preds\n",
    "\n",
    "    def train_and_test(self):\n",
    "        self.model.fit()\n",
    "        gk_preds, not_gk_preds = self.model.test()\n",
    "        test = self.input_data.all_test_data()\n",
    "        submit = pd.read_csv(self.submit_file_path)\n",
    "        test.loc[test['is_gk'] == True, 'pred'] = gk_preds\n",
    "        test.loc[test['is_gk'] == False, 'pred'] = not_gk_preds\n",
    "\n",
    "        submit['y'] = np.array(test['pred'])\n",
    "        submit.to_csv(self.prediction_file_path, index=False)\n",
    "\n",
    "\n",
    "class RunnerBase():\n",
    "\n",
    "    def _build(self):\n",
    "        return None\n",
    "\n",
    "    def _trainer(self, gk_features, not_gk_features=None, eval_ratio=0.2):\n",
    "        return Trainer(self._build, gk_features, not_gk_features, eval_ratio)\n",
    "\n",
    "    def train(self, gk_features, not_gk_features=None):\n",
    "        trainer = self._trainer(gk_features, not_gk_features)\n",
    "        print(trainer.train()[0])\n",
    "\n",
    "    def train_and_test(self, gk_features, not_gk_features=None):\n",
    "        self._trainer(gk_features, not_gk_features, 0).train_and_test()\n",
    "\n",
    "\n",
    "class XGBoostRunner(RunnerBase):\n",
    "    def __init__(self, max_depth=8):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def _build(self):\n",
    "        return xgb.XGBRegressor(max_depth=self.max_depth, learning_rate=0.1, n_estimators=160, silent=False,\n",
    "                                objective='reg:gamma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['height_cm', 'weight_kg', 'potential', 'BMI', 'pac',\n",
    "                           'phy', 'international_reputation', 'age', 'best_pos']\n",
    "def train1():\n",
    "    XGBoostRunner().train(features)\n",
    "def train_and_test1():\n",
    "    XGBoostRunner().train_and_test(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdata=InputData(train_data_path='../dataset/fifa2018/train.csv',test_data_path='../dataset/fifa2018/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>club</th>\n",
       "      <th>league</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>nationality</th>\n",
       "      <th>potential</th>\n",
       "      <th>pac</th>\n",
       "      <th>sho</th>\n",
       "      <th>...</th>\n",
       "      <th>cm</th>\n",
       "      <th>cdm</th>\n",
       "      <th>cb</th>\n",
       "      <th>lb</th>\n",
       "      <th>gk</th>\n",
       "      <th>y</th>\n",
       "      <th>age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>is_gk</th>\n",
       "      <th>best_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6935</th>\n",
       "      <td>6935</td>\n",
       "      <td>308</td>\n",
       "      <td>28</td>\n",
       "      <td>1992-03-13</td>\n",
       "      <td>174</td>\n",
       "      <td>71</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>77</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.106849</td>\n",
       "      <td>23.450918</td>\n",
       "      <td>False</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>3660</td>\n",
       "      <td>369</td>\n",
       "      <td>26</td>\n",
       "      <td>1984-02-29</td>\n",
       "      <td>191</td>\n",
       "      <td>90</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>54</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.147945</td>\n",
       "      <td>24.670376</td>\n",
       "      <td>False</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6520</th>\n",
       "      <td>6520</td>\n",
       "      <td>511</td>\n",
       "      <td>39</td>\n",
       "      <td>1991-04-29</td>\n",
       "      <td>177</td>\n",
       "      <td>70</td>\n",
       "      <td>49</td>\n",
       "      <td>72</td>\n",
       "      <td>71</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>330.0</td>\n",
       "      <td>26.980822</td>\n",
       "      <td>22.343516</td>\n",
       "      <td>False</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>7995</td>\n",
       "      <td>93</td>\n",
       "      <td>6</td>\n",
       "      <td>1995-10-21</td>\n",
       "      <td>180</td>\n",
       "      <td>76</td>\n",
       "      <td>111</td>\n",
       "      <td>71</td>\n",
       "      <td>65</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.498630</td>\n",
       "      <td>23.456790</td>\n",
       "      <td>False</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8126</th>\n",
       "      <td>8126</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>1991-12-30</td>\n",
       "      <td>187</td>\n",
       "      <td>85</td>\n",
       "      <td>110</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320.0</td>\n",
       "      <td>26.309589</td>\n",
       "      <td>24.307244</td>\n",
       "      <td>False</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  club  league birth_date  height_cm  weight_kg  nationality  \\\n",
       "6935  6935   308      28 1992-03-13        174         71            5   \n",
       "3660  3660   369      26 1984-02-29        191         90           64   \n",
       "6520  6520   511      39 1991-04-29        177         70           49   \n",
       "7995  7995    93       6 1995-10-21        180         76          111   \n",
       "8126  8126   141      20 1991-12-30        187         85          110   \n",
       "\n",
       "      potential  pac  sho  ...    cm   cdm    cb    lb  gk      y        age  \\\n",
       "6935         64   77   58  ...  59.0  52.0  48.0  52.0 NaN   26.0  26.106849   \n",
       "3660         66   54   39  ...  52.0  65.0  69.0  65.0 NaN   26.0  34.147945   \n",
       "6520         72   71   61  ...  71.0  69.0  65.0  67.0 NaN  330.0  26.980822   \n",
       "7995         71   65   49  ...  57.0  58.0  55.0  56.0 NaN   20.0  22.498630   \n",
       "8126         76   77   70  ...  54.0  43.0  42.0  45.0 NaN  320.0  26.309589   \n",
       "\n",
       "            BMI  is_gk  best_pos  \n",
       "6935  23.450918  False      65.0  \n",
       "3660  24.670376  False      69.0  \n",
       "6520  22.343516  False      71.0  \n",
       "7995  23.456790  False      58.0  \n",
       "8126  24.307244  False      70.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=['height_cm', 'weight_kg', 'potential', 'BMI', 'pac',\n",
    "                           'phy', 'international_reputation', 'age', 'best_pos']\n",
    "inputdata.train_data(False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer()\n",
    "imputer.fit(traindata.loc[:, ['rw', 'st', 'lw', 'cf', 'cam', 'cm']])\n",
    "x_new = imputer.transform(traindata.loc[:, ['rw', 'st', 'lw', 'cf', 'cam', 'cm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_indicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Imputation transformer for completing missing values.\n",
       "\n",
       "Read more in the :ref:`User Guide <impute>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "missing_values : number, string, np.nan (default) or None\n",
       "    The placeholder for the missing values. All occurrences of\n",
       "    `missing_values` will be imputed.\n",
       "\n",
       "strategy : string, optional (default=\"mean\")\n",
       "    The imputation strategy.\n",
       "\n",
       "    - If \"mean\", then replace missing values using the mean along\n",
       "      each column. Can only be used with numeric data.\n",
       "    - If \"median\", then replace missing values using the median along\n",
       "      each column. Can only be used with numeric data.\n",
       "    - If \"most_frequent\", then replace missing using the most frequent\n",
       "      value along each column. Can be used with strings or numeric data.\n",
       "    - If \"constant\", then replace missing values with fill_value. Can be\n",
       "      used with strings or numeric data.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "       strategy=\"constant\" for fixed value imputation.\n",
       "\n",
       "fill_value : string or numerical value, optional (default=None)\n",
       "    When strategy == \"constant\", fill_value is used to replace all\n",
       "    occurrences of missing_values.\n",
       "    If left to the default, fill_value will be 0 when imputing numerical\n",
       "    data and \"missing_value\" for strings or object data types.\n",
       "\n",
       "verbose : integer, optional (default=0)\n",
       "    Controls the verbosity of the imputer.\n",
       "\n",
       "copy : boolean, optional (default=True)\n",
       "    If True, a copy of X will be created. If False, imputation will\n",
       "    be done in-place whenever possible. Note that, in the following cases,\n",
       "    a new copy will always be made, even if `copy=False`:\n",
       "\n",
       "    - If X is not an array of floating values;\n",
       "    - If X is encoded as a CSR matrix;\n",
       "    - If add_indicator=True.\n",
       "\n",
       "add_indicator : boolean, optional (default=False)\n",
       "    If True, a `MissingIndicator` transform will stack onto output\n",
       "    of the imputer's transform. This allows a predictive estimator\n",
       "    to account for missingness despite imputation. If a feature has no\n",
       "    missing values at fit/train time, the feature won't appear on\n",
       "    the missing indicator even if there are missing values at\n",
       "    transform/test time.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "statistics_ : array of shape (n_features,)\n",
       "    The imputation fill value for each feature.\n",
       "\n",
       "indicator_ : :class:`sklearn.impute.MissingIndicator`\n",
       "    Indicator used to add binary indicators for missing values.\n",
       "    ``None`` if add_indicator is False.\n",
       "\n",
       "See also\n",
       "--------\n",
       "IterativeImputer : Multivariate imputation of missing values.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.impute import SimpleImputer\n",
       ">>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
       ">>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
       "... # doctest: +NORMALIZE_WHITESPACE\n",
       "SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "        missing_values=nan, strategy='mean', verbose=0)\n",
       ">>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n",
       ">>> print(imp_mean.transform(X))\n",
       "... # doctest: +NORMALIZE_WHITESPACE\n",
       "[[ 7.   2.   3. ]\n",
       " [ 4.   3.5  6. ]\n",
       " [10.   3.5  9. ]]\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Columns which only contained missing values at `fit` are discarded upon\n",
       "`transform` if strategy is not \"constant\".\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/anaconda3-5.3.0/envs/pytorch/lib/python3.6/site-packages/sklearn/impute/_base.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SimpleImputer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6666666666666667"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(a-b).sum()/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m            ndarray\n",
       "\u001b[0;31mString form:\u001b[0m     [1 4 5]\n",
       "\u001b[0;31mLength:\u001b[0m          3\n",
       "\u001b[0;31mFile:\u001b[0m            ~/.pyenv/versions/anaconda3-5.3.0/envs/pytorch/lib/python3.6/site-packages/numpy/__init__.py\n",
       "\u001b[0;31mDocstring:\u001b[0m       <no docstring>\n",
       "\u001b[0;31mClass docstring:\u001b[0m\n",
       "ndarray(shape, dtype=float, buffer=None, offset=0,\n",
       "        strides=None, order=None)\n",
       "\n",
       "An array object represents a multidimensional, homogeneous array\n",
       "of fixed-size items.  An associated data-type object describes the\n",
       "format of each element in the array (its byte-order, how many bytes it\n",
       "occupies in memory, whether it is an integer, a floating point number,\n",
       "or something else, etc.)\n",
       "\n",
       "Arrays should be constructed using `array`, `zeros` or `empty` (refer\n",
       "to the See Also section below).  The parameters given here refer to\n",
       "a low-level method (`ndarray(...)`) for instantiating an array.\n",
       "\n",
       "For more information, refer to the `numpy` module and examine the\n",
       "methods and attributes of an array.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "(for the __new__ method; see Notes below)\n",
       "\n",
       "shape : tuple of ints\n",
       "    Shape of created array.\n",
       "dtype : data-type, optional\n",
       "    Any object that can be interpreted as a numpy data type.\n",
       "buffer : object exposing buffer interface, optional\n",
       "    Used to fill the array with data.\n",
       "offset : int, optional\n",
       "    Offset of array data in buffer.\n",
       "strides : tuple of ints, optional\n",
       "    Strides of data in memory.\n",
       "order : {'C', 'F'}, optional\n",
       "    Row-major (C-style) or column-major (Fortran-style) order.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "T : ndarray\n",
       "    Transpose of the array.\n",
       "data : buffer\n",
       "    The array's elements, in memory.\n",
       "dtype : dtype object\n",
       "    Describes the format of the elements in the array.\n",
       "flags : dict\n",
       "    Dictionary containing information related to memory use, e.g.,\n",
       "    'C_CONTIGUOUS', 'OWNDATA', 'WRITEABLE', etc.\n",
       "flat : numpy.flatiter object\n",
       "    Flattened version of the array as an iterator.  The iterator\n",
       "    allows assignments, e.g., ``x.flat = 3`` (See `ndarray.flat` for\n",
       "    assignment examples; TODO).\n",
       "imag : ndarray\n",
       "    Imaginary part of the array.\n",
       "real : ndarray\n",
       "    Real part of the array.\n",
       "size : int\n",
       "    Number of elements in the array.\n",
       "itemsize : int\n",
       "    The memory use of each array element in bytes.\n",
       "nbytes : int\n",
       "    The total number of bytes required to store the array data,\n",
       "    i.e., ``itemsize * size``.\n",
       "ndim : int\n",
       "    The array's number of dimensions.\n",
       "shape : tuple of ints\n",
       "    Shape of the array.\n",
       "strides : tuple of ints\n",
       "    The step-size required to move from one element to the next in\n",
       "    memory. For example, a contiguous ``(3, 4)`` array of type\n",
       "    ``int16`` in C-order has strides ``(8, 2)``.  This implies that\n",
       "    to move from element to element in memory requires jumps of 2 bytes.\n",
       "    To move from row-to-row, one needs to jump 8 bytes at a time\n",
       "    (``2 * 4``).\n",
       "ctypes : ctypes object\n",
       "    Class containing properties of the array needed for interaction\n",
       "    with ctypes.\n",
       "base : ndarray\n",
       "    If the array is a view into another array, that array is its `base`\n",
       "    (unless that array is also a view).  The `base` array is where the\n",
       "    array data is actually stored.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "array : Construct an array.\n",
       "zeros : Create an array, each element of which is zero.\n",
       "empty : Create an array, but leave its allocated memory unchanged (i.e.,\n",
       "        it contains \"garbage\").\n",
       "dtype : Create a data-type.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "There are two modes of creating an array using ``__new__``:\n",
       "\n",
       "1. If `buffer` is None, then only `shape`, `dtype`, and `order`\n",
       "   are used.\n",
       "2. If `buffer` is an object exposing the buffer interface, then\n",
       "   all keywords are interpreted.\n",
       "\n",
       "No ``__init__`` method is needed because the array is fully initialized\n",
       "after the ``__new__`` method.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "These examples illustrate the low-level `ndarray` constructor.  Refer\n",
       "to the `See Also` section above for easier ways of constructing an\n",
       "ndarray.\n",
       "\n",
       "First mode, `buffer` is None:\n",
       "\n",
       ">>> np.ndarray(shape=(2,2), dtype=float, order='F')\n",
       "array([[0.0e+000, 0.0e+000], # random\n",
       "       [     nan, 2.5e-323]])\n",
       "\n",
       "Second mode:\n",
       "\n",
       ">>> np.ndarray((2,), buffer=np.array([1,2,3]),\n",
       "...            offset=np.int_().itemsize,\n",
       "...            dtype=int) # offset = 1*itemsize, i.e. skip first element\n",
       "array([2, 3])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}